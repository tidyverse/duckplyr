---
title: "Large data"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{01 Large data}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
clean_output <- function(x, options) {
  x <- gsub("0x[0-9a-f]+", "0xdeadbeef", x)
  x <- gsub("dataframe_[0-9]*_[0-9]*", "      dataframe_42_42      ", x)
  x <- gsub("[0-9]*\\.___row_number ASC", "42.___row_number ASC", x)
  x <- gsub("â”€", "-", x)
  x
}

local({
  hook_source <- knitr::knit_hooks$get("document")
  knitr::knit_hooks$set(document = clean_output)
})

knitr::opts_chunk$set(
  collapse = TRUE,
  eval = identical(Sys.getenv("IN_PKGDOWN"), "true") || (getRversion() >= "4.1"),
  comment = "#>"
)

options(conflicts.policy = list(warn = FALSE))

Sys.setenv(DUCKPLYR_FALLBACK_COLLECT = 0)
```

This vignette discusses how to work with large data sets using duckplyr.

```{r attach}
library(conflicted)
library(duckplyr)
conflict_prefer("filter", "dplyr")
```


## Introduction

Data frames and other objects in R are stored in RAM, which can be a bottleneck when working with large data sets.
A variety of tools have been developed to work with large data sets, also in R.
One examples is the dbplyr package, a dplyr backend that connects to SQL databases and is designed to work with various databases that support SQL.
This is a viable approach if the data is already stored in a database, or if the data is stored in Parquet or CSV files and loaded as a lazy table via `duckdb::tbl_file()`.

The dbplyr package translates dplyr code to SQL.
The syntax and semantics are very similar, but not identical to plain dplyr.
In contrast, the duckplyr package aims to be a fully compatible drop-in replacement for dplyr, with *exactly* the same syntax and semantics:

- Input and output are data frames or tibbles.
- All dplyr verbs are supported, with fallback.
- All R data types and functions are supported, with fallback.
- No SQL is generated, instead, DuckDB's "relational" interface is used.

Full compatibility means fewer surprises and less cognitive load for the user.
With DuckDB as the backend, duckplyr can also handle large data sets that do not fit into RAM, keeping full dplyr compatibility.
The tools for bringing data into and out of R memory are modeled after the dplyr and dbplyr packages, and are described in the following sections.

See `vignette("limits")` for limitations in the translation employed by duckplyr, and `?fallback` for more information on fallback.

## To duckplyr

The `duckdb_tibble()` function creates a duckplyr data frame from vectors:

```{r}
df <- duckdb_tibble(x = 1:5, y = letters[1:5])
df
```

The `duckdb_tibble()` function is a drop-in replacement for `tibble()`, and can be used in the same way.

Similarly, `as_duckdb_tibble()` can be used to convert a data frame or another object to a duckplyr data frame:

```{r}
flights_df() |> 
  as_duckdb_tibble()
```

Existing code that uses DuckDB via dbplyr can also take advantage:

```{r}
con <- DBI::dbConnect(duckdb::duckdb())
DBI::dbWriteTable(con, "data", data.frame(x = 1:5, y = letters[1:5]))

dbplyr_data <- tbl(con, "data")
dbplyr_data
dbplyr_data |> 
  explain()
dbplyr_data |>
  as_duckdb_tibble()
dbplyr_data |>
  as_duckdb_tibble() |> 
  explain()
```
