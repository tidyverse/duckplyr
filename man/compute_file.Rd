% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/compute_file.R
\name{compute_file}
\alias{compute_file}
\alias{compute_parquet}
\alias{compute_csv}
\title{Compute results to a file}
\usage{
compute_parquet(x, path, ..., funnel = NULL, options = NULL)

compute_csv(x, path, ..., funnel = NULL, options = NULL)
}
\arguments{
\item{x}{A data frame, data frame extension (e.g. a tibble), or a lazy
data frame (e.g. from dbplyr or dtplyr). See \emph{Methods}, below, for more
details.}

\item{path}{The path to store the result in.}

\item{...}{These dots are for future extensions and must be empty.}

\item{funnel}{Either a logical:
\itemize{
\item Set to \code{TRUE} to return a funneled data frame.
\item Set to \code{FALSE} to return an unfunneled data frame.
}

Or a named vector with at least one of
\itemize{
\item \code{cells} (numeric)
\item \code{rows} (numeric)
}

to allow materialization for data up to a certain size,
measured in cells (values) and rows in the resulting data frame.

If \code{cells} is specified but not \code{rows}, \code{rows} is \code{Inf}.
If \code{rows} is specified but not \code{cells}, \code{cells} is \code{Inf}.

The default is to inherit the funneling of the input.
see the "Funneling" section.}

\item{options}{A list of additional options to pass to create the storage format,
see \url{https://duckdb.org/docs/data/parquet/overview#writing-to-parquet-files}
or \url{https://duckdb.org/docs/data/csv/overview#writing-using-the-copy-statement}
for details.}
}
\description{
These functions apply to (funneled) duckplyr frames.
They executes a query and stores the results in a flat file.
The result is a duckplyr frame that can be used with subsequent dplyr verbs.

\code{compute_parquet()} creates a Parquet file.

\code{compute_csv()} creates a CSV file.
}
\section{Funneling}{

Data frames backed by duckplyr, with class \code{"duckplyr_df"},
behave as regular data frames in almost all respects.
In particular, direct column access like \code{df$x},
or retrieving the number of rows with \code{\link[=nrow]{nrow()}}, works identically.
Conceptually, duckplyr frames are "eager": from a user's perspective,
they behave like regular data frames.
Under the hood, two key differences provide improved performance and usability:
lazy materialization and funneling.

For a duckplyr frame that is the result of a dplyr operation,
accessing column data or retrieving the number of rows will trigger a computation
that is carried out by DuckDB, not dplyr.
In this sense, duckplyr frames are also "lazy":
the computation is deferred until the last possible moment,
allowing DuckDB to optimize the whole pipeline.
This is similar to lazy tables in \pkg{dbplyr} and \pkg{dtplyr},
but different from \pkg{dplyr} where each intermediate step is computed.

Being both "eager" and "lazy" at the same time introduces a challenge:
it is too easy to accidentally trigger computation,
which may be prohibitive if an intermediate result is too large.
This is where funneling comes in.
\itemize{
\item For unfunneled duckplyr frames, the underlying DuckDB computation is carried out
upon the first request.
Once the results are computed, they are cached and subsequent requests are fast.
This is a good choice for small to medium-sized data,
where DuckDB can provide a nice speedup but materializing the data is affordable
at any stage.
This is the default for \code{duckdb_tibble()} and \code{as_duckdb_tibble()}.
\item For funneled duckplyr frames, accessing a column or requesting the number of rows
triggers an error, either unconditionally, or if the result exceeds a certain size.
This is a good choice for large data sets where the cost of materializing the data
may be prohibitive due to size or computation time,
and the user wants to control when the computation is carried out.
The default for the ingestion functions like \code{\link[=read_parquet_duckdb]{read_parquet_duckdb()}}
is to limit the result size to one million cells (values in the resulting data frame).
}

Funneled duckplyr frames behave like \href{https://dtplyr.tidyverse.org/reference/lazy_dt.html}{\code{dtplyr}'s lazy frames},
or dbplyr's lazy frames:
the computation only starts when you \strong{explicitly} request it with a "collect"
function.
In dtplyr and dbplyr, there are no unfunneled frames: collection always needs to be
explicit.

A funneled duckplyr frame can be converted to an unfunneled one with \code{as_duckdb_tibble(funnel = "open")}.
The \code{\link[=collect.duckplyr_df]{collect.duckplyr_df()}} method triggers computation and converts to a plain tibble.
Other useful methods include \code{\link[=compute_file]{compute_file()}} for storing results in a file,
and \code{\link[=compute.duckplyr_df]{compute.duckplyr_df()}} for storing results in temporary storage on disk.

Beyond safety regarding memory usage, funneled frames also allow you
to check that all operations are supported by DuckDB:
for a funneled frame with \code{funnel = "closed"}, fallbacks to dplyr are not possible.
As a reminder, computing via DuckDB is currently not always possible,
see \code{vignette("limits")} for the supported operations.
In such cases, the original dplyr implementation is used, see \link{fallback} for details.
As the original dplyr implementation accesses columns directly,
the data must be materialized before a fallback can be executed.
This means that automatic fallback is only possible for "unfunneled" duckplyr frames,
while for "funneled" duckplyr frames, one of the aforementioned collection methods must be used first.
}

\examples{
library(duckplyr)
df <- data.frame(x = c(1, 2))
df <- mutate(df, y = 2)
path <- tempfile(fileext = ".parquet")
df <- compute_parquet(df, path)
explain(df)
}
\seealso{
\code{\link[=compute.duckplyr_df]{compute.duckplyr_df()}}, \code{\link[dplyr:compute]{dplyr::collect()}}
}
